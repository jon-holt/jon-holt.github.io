<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Using pre-trained word embeddings for prediction :: Uninformed Prior — Jon Holt&#39;s blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="/post/using-word-embeddings-to-predict-donorschoose-acceptance/" />


<link rel="stylesheet" href="/assets/style.css">
<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Source+Code+Pro' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="/assets/style.css">




<link rel="apple-touch-icon-precomposed" sizes="144x144" href="img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="img/favicon.png">


<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Using pre-trained word embeddings for prediction :: Uninformed Prior — Jon Holt&#39;s blog" />
<meta name="twitter:description" content="Introduction In my last post I introduced the DonorsChoose dataset, which can be easily downloaded from Kaggle. This dataset is fun to play with because it has lots of natural language processing (NLP) potential. Each observation contains (among other things) essays written by classroom teachers. Just like in my last post, we’ll focus on the essays here.
This time, we’ll explore the teacher essays using word embeddings. Word embeddings are a mathematical representation of word similarity (e." />
<meta name="twitter:site" content="" />
<meta name="twitter:creator" content="" />
<meta name="twitter:image" content="/img/default.jpg">


<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Using pre-trained word embeddings for prediction :: Uninformed Prior — Jon Holt&#39;s blog">
<meta property="og:description" content="" />
<meta property="og:url" content="/post/using-word-embeddings-to-predict-donorschoose-acceptance/" />
<meta property="og:site_name" content="Using pre-trained word embeddings for prediction" />
<meta property="og:image" content="/img/default.jpg">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">
<meta property="article:section" content="NLP" /><meta property="article:section" content="word embeddings" /><meta property="article:section" content="R" />
<meta property="article:published_time" content="2018-10-24 00:00:00 &#43;0000 UTC" />







</head>
<body class="dark-theme">
<div class="container">
  <header class="header">
  <span class="header__inner">
    <a href="/" style="text-decoration: none;">
  <div class="logo">
    
      <span class="logo__mark">></span>
      <span class="logo__text">uninformed prior</span>
      <span class="logo__cursor"></span>
    
  </div>
</a>
    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner">
    
      <li><a href="/about">About</a></li>
    
  </ul>
</nav>
        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none"/>
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
          </svg>        
        </span>
      
      <span class="theme-toggle">
        <svg class="bulb-off" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
  <rect width="24" height="24"/>
  <path d="M4 19C4 19.55 4.45 20 5 20H9C9.55 20 10 19.55 10 19V18H4V19ZM7 0C3.14 0 0 3.14 0 7C0 9.38 1.19 11.47 3 12.74V15C3 15.55 3.45 16 4 16H10C10.55 16 11 15.55 11 15V12.74C12.81 11.47 14 9.38 14 7C14 3.14 10.86 0 7 0ZM9.85 11.1L9 11.7V14H5V11.7L4.15 11.1C2.8 10.16 2 8.63 2 7C2 4.24 4.24 2 7 2C9.76 2 12 4.24 12 7C12 8.63 11.2 10.16 9.85 11.1Z" transform="translate(5 2)" fill="black"/>
</svg>

<svg class="bulb-on" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
  <rect width="24" height="24"/>
  <path class="bulb-on__base" d="M4 19C4 19.55 4.45 20 5 20H9C9.55 20 10 19.55 10 19V18H4V19Z" transform="translate(5 2)" fill="#a9a9b3" />
  <path class="bulb-on__glass" d="M0 7C0 3.14 3.14 0 7 0C10.86 0 14 3.14 14 7C14 9.38 12.81 11.47 11 12.74V15C11 15.55 10.55 16 10 16H4C3.45 16 3 15.55 3 15V12.74C1.19 11.47 0 9.38 0 7Z" transform="translate(5 2)" fill="#a9a9b3" />
</svg>
  
      </span>
    </span>
  </span>
</header>


  <div class="content">
    
  <div class="post">
    <h2 class="post-title"><a href="/post/using-word-embeddings-to-predict-donorschoose-acceptance/">Using pre-trained word embeddings for prediction</a></h2>
    <div class="post-meta">
      <span class="post-date">
        2018-10-24
      </span>
      <span class="post-author">Written by Jon</span>
    </div>

    

    

    <div class="post-content">
      <div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In my last post I introduced the DonorsChoose dataset, which can be easily downloaded from <a href="https://www.kaggle.com/c/donorschoose-application-screening">Kaggle</a>. This dataset is fun to play with because it has lots of natural language processing (NLP) potential. Each observation contains (among other things) essays written by classroom teachers. Just like in my last post, we’ll focus on the essays here.</p>
<p>This time, we’ll explore the teacher essays using <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. Word embeddings are a mathematical representation of word similarity (e.g. <em>dog</em> and <em>wolf</em> are similar; <em>dog</em> and <em>Paris</em> are not similar). Word embeddings can be created for any document or corpus, but are generally constructed from massive corpuses such as Wikipedia.</p>
<p>The way word embeddings are created is actually pretty simple: First, you construct a square matrix where the dimension is equal to the number of unique words you have. Each entrance to the matrix is calculated as the number of times the word on one axis occurs with the word on the other axis, within a certain window. So if the window size is 5, then you look at 5 words at a time. The window slides along the entire corpus and you count up the frequency of each word inside the sliding window, relative to other words. Finally, you take your big matrix and do an eigenvalue decomposition, just like principal components analysis, or empirical orthogonal functions, or any other synonym for matrix decomposition.</p>
<p>If you didn’t follow along with all that, just know this: word embeddings represent words in some mathematical vector space, in which similar words are close to one another in the vector space.</p>
<div class="figure">
<img src="/images/duh.jpg" width="750" />

</div>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<p>First, load up some useful packages.</p>
<pre class="r"><code>library(tm)
library(softmaxreg)</code></pre>
<p>Next, we will pre-process the text. Common steps include removing puncuation, upper-case letters, extra spaces, and stop words.</p>
<pre class="r"><code># Create a volatile corpus
docs.s &lt;- Corpus(VectorSource(df$project_essay_2))

# remove line breaks
line_break &lt;- function(x) gsub(&quot;\\\\r\\\\n&quot;, &quot; &quot;, x) 
docs.s &lt;- tm_map(docs.s, line_break)

# remove stop words
docs.s &lt;- tm_map(docs.s, removeWords, stopwords(&quot;english&quot;))

# remove upper-case
docs.s &lt;- tm_map(docs.s, content_transformer(tolower))

# remove punctuation
docs.s &lt;- tm_map(docs.s, removePunctuation, preserve_intra_word_dashes = TRUE)

# remove numbers
docs.s &lt;- tm_map(docs.s, removeNumbers)

# remove left over &quot;th&quot; from numbers
docs.s &lt;- tm_map(docs.s, removeWords, c(&quot;th&quot;))

# remove white Space
docs.s &lt;- tm_map(docs.s, stripWhitespace)</code></pre>
<p>Next, let’s load the pre-trained word vectors. These word vectors were trained on Wikipedia, so we can be confident that the vector space accurately captures word similaries in the English language.</p>
<pre class="r"><code># function to load pretrained word vectors
    proc_pretrained_vec &lt;- function(p_vec) {

        # initialize space for values and the names of each word in vocabulary
        vals &lt;- vector(mode = &quot;list&quot;, length(p_vec))
        names &lt;- character(length(p_vec))

        # loop through to gather values and names of each word
        for(i in 1:length(p_vec)) {
            if(i %% 1000 == 0) {print(i)}
            this_vec &lt;- p_vec[i]
            this_vec_unlisted &lt;- unlist(strsplit(this_vec, &quot; &quot;))
            this_vec_values &lt;- as.numeric(this_vec_unlisted[-1])  # this needs testing, does it become numeric?
            this_vec_name &lt;- this_vec_unlisted[1]

            vals[[i]] &lt;- this_vec_values
            names[[i]] &lt;- this_vec_name
        }

        # convert lists to data.frame and attach the names
        glove &lt;- data.frame(vals)
        names(glove) &lt;- names

        return(glove)
    }</code></pre>
<p>I’ll upload the 50-dimension vectors. 50-dimension means that the words are projected into a 50-dimensional space, in which similar words are close to one another. It’s impossible to visualize what 50 dimensions looks like (humans can only picture a 3-dimensional space), so you just have to use your imagination. In addition to the 50-dimension vectors, you can also obtain 100 and 200-dimension vectors. <a href="https://nlp.stanford.edu/projects/glove/">Here’s the link</a>. Loading the 50-dimension vectors takes about 2 and a half minutes on my Macbook.</p>
<pre class="r"><code># load pretrained glove
g6b_50 &lt;- scan(file = &quot;/glove.6B/glove.6B.50d.txt&quot;, what=&quot;&quot;, sep=&quot;\n&quot;)

# call the function to convert the raw GloVe vector to data.frame (extra lines are for wall-time reporting)
t_temp &lt;- Sys.time()
glove.50 &lt;- proc_pretrained_vec(g6b_50)  # this is the actual function call
(t_elap_temp &lt;- paste0(round(as.numeric(Sys.time() - t_temp, units=&quot;mins&quot;), digits = 2), &quot; minutes&quot;))
    
print(dim(glove.50))</code></pre>
<p>Oops, looks like the rows are the word vectors and the columns are the words. I think it’s more intuitive to have it the other way around, so let’s transpose the matrix. We’ll also make it a dataframe.</p>
<pre class="r"><code>vec_pre &lt;- t(glove.50)
ff &lt;- data.frame(names = row.names(vec_pre), vec_pre)</code></pre>
<p>Now we have our word vectors. There are 400,000 rows - one row for each unique word. There are 50 columns - one column for each dimension of the vector space. Let’s look at the euclidean distane between some of the row vectors. For example, let’s compare the distances between the following two pairs: “teacher” and “student”; and “teacher” and “mountain”.</p>
<p>What do we expect the difference will be? Well, the vectors are the locations of words in a vector space representative of word co-occurance (similarity). So, we expect that the distance between “teacher” and “student” will be less than the distance between “teacher” and “mountain”.</p>
<pre class="r"><code>teacher &lt;- ff[ff$names==&quot;teacher&quot;,2:51]
student &lt;- ff[ff$names==&quot;student&quot;,2:51]
mountain &lt;- ff[ff$names==&quot;mountain&quot;,2:51]

# distance between teacher and student
print(&quot;distance between teacher and student:&quot;)
sqrt(sum((teacher-student)^2))

# distance between teacher and mountain
print(&quot;distance between teacher and mountain:&quot;)
sqrt(sum((teacher-mountain)^2))</code></pre>
<pre><code>## [1] &quot;distance between teacher and student:&quot;</code></pre>
<pre><code>## [1] 2.32</code></pre>
<pre><code>## [1] &quot;distance between teacher and mountain:&quot;</code></pre>
<pre><code>## [1] 6.62</code></pre>
<p>Cool! It looks like we’re getting some meaningful information from the word vectors.</p>
<div class="figure">
<img src="/images/Clap.jpg" width="0" />

</div>
<p>How might we make use of the word vectors for a real-life application?</p>
<p>The teacher essay dataset also has a column that specifies whether each teacher received funding for their classroom. We can run a logistic regression model in which the word vectors are the predictors, and the classroom funding is the response. For this model to work, there are two underlying assumptions. First, we assume that teachers who write a “good” essay will get funding, while “bad”&quot; essays will not get funding. Second, we assume that the word vectors will be able to capture the essence of a “good” essay.</p>
<p>Let’s load up the glmnet package, which is great for running a quick lasso regression. How does the model do?</p>
<pre class="r"><code>library(glmnet)
glmnet_classifier = cv.glmnet(x = sumVecs, y = df$project_is_approved,
                 family = &#39;binomial&#39;, 
                 alpha = 1, #l1 penalty
                 type.measure = &quot;auc&quot;,
                 nfolds = 10,
                 thresh = 1e-3,
                 maxit = 1e3)

plot(glmnet_classifier)</code></pre>
<div class="figure">
<img src="/images/word_vectors_AUC.jpeg" width="750" />

</div>
<p>Hey, not bad! We got an AUC of almost 0.70 using the essays alone. That means that (1) the quality of the teacher essays was captured by the word vectors and (2) the word vectors can accurately predict whether or not a teacher’s essay was good enough to get funded. Pretty cool!</p>
<div class="figure">
<img src="/images/sweet.jpg" width="750" />

</div>
</div>

    </div>
    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>
        <div class="pagination__buttons">
          
          
            <a class="btn previous" href="/post/how-do-teachers-describe-their-students/">How do teachers describe their students? →</a>
           
        </div>
      </div>
    
  </div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <a href="/" style="text-decoration: none;">
  <div class="logo">
    
      <span class="logo__mark">></span>
      <span class="logo__text">uninformed prior</span>
      <span class="logo__cursor"></span>
    
  </div>
</a>
      <div class="copyright">
        <span>Powered by <a href="http://gohugo.io">Hugo</a></span>
        <span>Theme created by <a href="https://twitter.com/panr">panr</a> 2018</span>
      </div>
    
  </div>
</footer>


  <script src="/assets/main.js"></script>

  <script src="/assets/prism.js"></script>



  
</div>

</body>
</html>
