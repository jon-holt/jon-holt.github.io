---
title: Using pre-trained word embeddings for prediction
author: Jon
date: '2018-10-24'
slug: using-word-embeddings-to-predict-donorschoose-acceptance
categories:
  - NLP
  - word embeddings
  - R
tags: []
---

```{r, echo=FALSE, eval=F}
load("/Users/Jon/Documents/Duke/DonorsChoose/PreProcessedTrainSet_Essay12.Rda")
```

## Introduction

In my last post I introduced the DonorsChoose dataset, which can be easily downloaded from [Kaggle](https://www.kaggle.com/c/donorschoose-application-screening). This dataset is fun to play with because it has lots of natural language processing (NLP) potential. Each observation contains (among other things) essays written by classroom teachers. Just like in my last post, we'll focus on the essays here. 

Here, we'll explore the teacher essays using [word embeddings](https://en.wikipedia.org/wiki/Word_embedding). Word embeddings are a mathematical representation of word similarity (e.g. *dog* and *wolf* are similar; *dog* and *Paris* are not). Word embeddings can be created for any document or corpus, but are generally constructed from massive corpuses such as Wikipedia. 

The way word embeddings are created is actually pretty simple: First, you construct a square matrix where the dimension is equal to the number of unique words you have. Each entrance to the matrix is calculated as the number of times the word on one axis occurs with the word on the other axis, within a certain window. So if the window size is 5, then you look at 5 words at a time. The window slides along the entire corpus and you count up the frequency of each word inside the sliding window, relative to other words. Finally, you take your big matrix and do an eigenvalue decomposition, just like principal components analysis, or empirical orthogonal functions, or any other synonym for matrix decomposition. 

If you didn't follow along with all that, just know this: word embeddings represent words in some mathematical vector space, in which similar words are close to one another in the vector space. 

![](/images/duh.jpg){width=750px}

## Analysis

First, load up some useful packages.

```{r, eval=F}
library(tm)
library(softmaxreg)
```

Next, we will pre-process the text. Common steps include removing puncuation, upper-case letters, extra spaces, and stop words. 

```{r, eval=F}
# Create a volatile corpus
docs.s <- Corpus(VectorSource(df$project_essay_2))

# remove line breaks
line_break <- function(x) gsub("\\\\r\\\\n", " ", x) 
docs.s <- tm_map(docs.s, line_break)

# remove stop words
docs.s <- tm_map(docs.s, removeWords, stopwords("english"))

# remove upper-case
docs.s <- tm_map(docs.s, content_transformer(tolower))

# remove punctuation
docs.s <- tm_map(docs.s, removePunctuation, preserve_intra_word_dashes = TRUE)

# remove numbers
docs.s <- tm_map(docs.s, removeNumbers)

# remove left over "th" from numbers
docs.s <- tm_map(docs.s, removeWords, c("th"))

# remove white Space
docs.s <- tm_map(docs.s, stripWhitespace)
```

Next, let's load the pre-trained word vectors. These word vectors were trained on Wikipedia, so we can be confident that the vector space accurately captures word similaries in the English language. 

```{r, eval=F}
# function to load pretrained word vectors
    proc_pretrained_vec <- function(p_vec) {

        # initialize space for values and the names of each word in vocabulary
        vals <- vector(mode = "list", length(p_vec))
        names <- character(length(p_vec))

        # loop through to gather values and names of each word
        for(i in 1:length(p_vec)) {
            if(i %% 1000 == 0) {print(i)}
            this_vec <- p_vec[i]
            this_vec_unlisted <- unlist(strsplit(this_vec, " "))
            this_vec_values <- as.numeric(this_vec_unlisted[-1])  # this needs testing, does it become numeric?
            this_vec_name <- this_vec_unlisted[1]

            vals[[i]] <- this_vec_values
            names[[i]] <- this_vec_name
        }

        # convert lists to data.frame and attach the names
        glove <- data.frame(vals)
        names(glove) <- names

        return(glove)
    }
```

I'll upload the 50-dimension vectors. They also have 100 and 200-dimension vectors. Loading the 50-dimension vectors takes about 2 and a half minutes on my Macbook. [Here's the link](https://nlp.stanford.edu/projects/glove/) to download the word vectors. 

```{r, eval=F}
# load pretrained glove
g6b_50 <- scan(file = "/Users/Jon/Documents/Duke/DonorsChoose/glove.6B/glove.6B.50d.txt", what="", sep="\n")

# call the function to convert the raw GloVe vector to data.frame (extra lines are for wall-time reporting)

t_temp <- Sys.time()
glove.50 <- proc_pretrained_vec(g6b_50)  # this is the actual function call
(t_elap_temp <- paste0(round(as.numeric(Sys.time() - t_temp, units="mins"), digits = 2), " minutes"))
    
print(dim(glove.50))
```

Oops, looks like the rows are the word vectors and the columns are the words. I think it's more intuitive to have it the other way around, so let's transpose the matrix. We'll also make it a dataframe.

```{r, eval=F}
vec_pre <- t(glove.50)
ff <- data.frame(names = row.names(vec_pre), vec_pre)
```

Now we have our word vectors. What to these mean anyway? Let's look at the euclidean distane between some of the row vectors. For example, let's compare the distances between the following two pairs: "teacher" and "student"; and "teacher" and "mountain".

What do we expect the difference will be? Well, the vectors are the locations of words in a latent space representative of word co-occurance. So, we expect that the distance between "teacher" and "student" will be less than the distance between "teacher" and "mountain". 

```{r, eval=F}
teacher <- ff[ff$names=="teacher",2:51]
student <- ff[ff$names=="student",2:51]
mountain <- ff[ff$names=="mountain",2:51]

# distance between teacher and student
print("distance between teacher and student:")
sqrt(sum((teacher-student)^2))

# distance between teacher and mountain
print("distance between teacher and mountain:")
sqrt(sum((teacher-mountain)^2))
```

```{r, echo=F}
print("distance between teacher and student:")
2.32
print("distance between teacher and mountain:")
6.62
```

Cool! It looks like we're getting some meaningful information from the word vectors. 

![](/images/Clap.jpg){width=500px}

How might we make use of the word vectors for a real-life application? Well, the teacher essay dataset also has a column that specifies whether each teacher received funding for their classroom. We can run a regression model in which the word vectors are the predictors, and the classroom funding is the response. For this model to work, there are two underlying assumptions. First, we assume that teachers who write a "good" essay will get funding, and vice versa. Second, we assume that the word vectors will be able to capture the essence of a "good" essay. 

Let's load up the glmnet package, which is great for running a quick lasso regression. How does the model do?

```{r, eval=F, echo=F}
load("word2vecSums_essay2_pre50.Rda")
dim(sumVecs)
```

```{r, eval=F}
library(glmnet)
glmnet_classifier = cv.glmnet(x = sumVecs, y = df$project_is_approved,
                 family = 'binomial', 
                 alpha = 1, #l1 penalty
                 type.measure = "auc",
                 nfolds = 10,
                 thresh = 1e-3,
                 maxit = 1e3)

plot(glmnet_classifier)
```

![](/images/word_vectors_AUC.jpeg){width=750px}

Hey, not bad! We got an AUC of almost 0.70 using the essays only. That means that (1) the quality of the teacher essays was captured by the word vectors and (2) the word vectors can accurately predict whether or not a teacher's essay was good enough to get funded. Pretty cool!

![](/images/sweet.jpg){width=750px}

```{r, echo=F, eval=F}
# convert corpus to dataframe
df <- data.frame(text=sapply(docs.s, identity), 
    stringsAsFactors=F)

t_temp <- Sys.time()
sumVecs <- matrix(nrow = dim(df)[1], ncol = 50) 
for (i in 1:dim(df)[1]) {
  d <- wordEmbed(object = df$text[i], dictionary = ff, meanVec = FALSE)
  sumVecs[i,] <- colSums(matrix(unlist(d), ncol = 50, byrow = FALSE))
}
(t_elap_temp <- paste0(round(as.numeric(Sys.time() - t_temp, units="mins"), digits = 2), " minutes"))

#save(ff, sumVecs, file = "word2vecSums_essay2_pre50.Rda") #put both the vector and the data in the same

load("word2vecSums_essay2_pre50.Rda")
```

