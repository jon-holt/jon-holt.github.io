<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Word Embeddings on Uninformed Prior</title>
    <link>https://jon-holt.github.io/categories/word-embeddings/</link>
    <description>Recent content in Word Embeddings on Uninformed Prior</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://jon-holt.github.io/categories/word-embeddings/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using pre-trained word embeddings for prediction</title>
      <link>https://jon-holt.github.io/post/using-word-embeddings-to-predict-donorschoose-acceptance/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jon-holt.github.io/post/using-word-embeddings-to-predict-donorschoose-acceptance/</guid>
      <description>Introduction In my last post I introduced the DonorsChoose dataset, which can be easily downloaded from Kaggle. This dataset is fun to play with because it has lots of natural language processing (NLP) potential. Each observation contains (among other things) essays written by classroom teachers. Just like in my last post, we’ll focus on the essays here.
This time, we’ll explore the teacher essays using word embeddings. Word embeddings are a mathematical representation of word similarity (e.</description>
    </item>
    
  </channel>
</rss>